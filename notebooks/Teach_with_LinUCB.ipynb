{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the learning algoritm which would be used. It has skipping, multi-threading enabled. \n",
    "\n",
    "# Put alpha = 0 & evaluate performance. In that case it would employ greedy strategy. Pick the arm with max expected reward. \n",
    "# Set confidenc_threshold = 50 to minimize skipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S_V   S_T   S_D   S_P   S_S  S_AT   S_L   S_A  S_SE  S_PA\n",
      "0    0.95  0.21  0.14  0.98  0.22  0.86  0.93  0.62  0.61  0.22\n",
      "1    0.11  0.19  0.45  0.65  0.20  0.51  0.27  0.34  0.12  0.92\n",
      "2    0.06  0.55  0.36  0.61  0.50  0.21  0.88  0.67  0.35  0.37\n",
      "3    0.94  0.80  0.44  0.33  0.79  0.88  0.13  0.97  0.75  0.35\n",
      "4    0.28  0.32  0.52  0.02  0.24  0.54  0.96  0.62  0.87  0.64\n",
      "5    0.78  0.34  0.02  0.02  0.58  0.42  0.67  0.32  0.13  1.00\n",
      "6    0.50  0.19  0.10  0.31  0.69  0.13  0.00  0.01  0.32  0.07\n",
      "7    0.40  0.99  0.13  0.46  0.61  0.55  0.90  0.40  0.90  0.90\n",
      "8    0.94  0.84  0.02  0.97  0.30  0.34  0.19  0.08  0.52  0.91\n",
      "9    0.08  0.20  0.01  0.62  0.12  0.28  0.93  0.52  0.67  0.68\n",
      "10   0.59  0.01  0.61  0.98  0.08  0.07  0.39  0.98  0.54  0.16\n",
      "11   0.99  0.12  0.10  0.69  0.30  0.98  0.19  0.10  0.22  0.77\n",
      "12   0.68  0.28  0.38  0.98  0.11  0.48  0.80  0.06  0.90  0.71\n",
      "13   0.33  0.82  0.34  0.97  0.09  0.12  0.07  0.98  0.06  0.80\n",
      "14   0.75  0.95  0.99  0.78  0.48  0.11  0.73  0.85  0.18  0.17\n",
      "15   0.43  0.60  0.85  0.92  0.88  0.51  0.50  0.15  0.74  0.66\n",
      "16   0.93  0.16  0.14  0.57  0.29  0.09  0.81  0.73  0.46  0.51\n",
      "17   0.78  0.23  0.57  0.09  0.50  0.21  0.91  0.24  0.79  0.17\n",
      "18   0.12  0.52  0.87  0.20  0.88  0.41  0.48  0.97  0.52  0.49\n",
      "19   0.73  0.30  0.92  0.43  0.20  0.51  0.80  0.33  0.38  0.12\n",
      "20   0.94  0.35  0.21  0.57  0.40  0.95  0.99  0.44  0.65  0.18\n",
      "21   0.96  0.89  0.30  0.45  0.81  0.43  0.66  0.14  0.78  0.73\n",
      "22   0.54  0.22  0.70  0.59  0.32  0.70  0.03  0.05  0.74  0.56\n",
      "23   0.08  0.75  0.89  0.51  0.25  0.18  0.40  0.37  0.10  0.47\n",
      "24   0.67  0.48  0.90  0.54  0.69  0.10  0.64  0.66  0.65  0.68\n",
      "25   0.75  0.17  0.05  0.64  0.91  0.74  0.43  0.70  0.47  0.28\n",
      "26   0.04  0.10  0.26  0.79  0.11  0.62  0.74  0.03  0.91  0.36\n",
      "27   0.71  0.57  0.62  0.77  0.74  0.89  0.79  0.29  0.18  0.91\n",
      "28   0.43  0.85  0.23  0.94  0.17  0.15  0.09  0.40  0.63  0.44\n",
      "29   0.94  0.73  0.94  0.80  0.26  0.51  0.55  0.39  0.80  0.72\n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
      "370  0.59  0.52  0.22  0.03  0.10  0.80  0.44  0.05  0.88  0.79\n",
      "371  0.38  0.22  0.58  0.04  0.81  0.72  0.21  0.93  0.03  0.51\n",
      "372  0.59  0.68  0.16  0.59  0.29  0.29  0.88  0.81  0.41  0.86\n",
      "373  0.91  0.37  0.16  0.28  0.84  0.07  0.23  0.55  0.84  0.59\n",
      "374  0.64  0.12  0.73  0.51  0.05  0.41  0.23  0.27  0.97  0.72\n",
      "375  0.77  0.03  0.88  0.30  0.07  0.55  0.02  0.57  0.71  0.79\n",
      "376  0.01  0.62  0.09  0.52  0.31  0.49  0.60  0.71  0.22  0.86\n",
      "377  0.20  0.19  0.13  0.95  0.09  0.69  0.59  0.45  0.52  0.28\n",
      "378  0.34  0.06  1.00  0.58  0.22  0.95  0.00  0.49  0.21  0.27\n",
      "379  0.58  0.54  0.42  0.22  0.63  0.42  0.23  0.22  0.09  0.46\n",
      "380  0.35  0.36  0.64  0.39  0.82  0.28  0.16  0.95  0.60  0.57\n",
      "381  0.83  0.18  0.94  0.44  0.34  0.94  0.60  0.88  0.50  0.10\n",
      "382  0.59  0.50  0.10  0.26  0.65  0.19  0.79  0.09  0.83  0.63\n",
      "383  0.44  0.45  0.51  0.48  0.34  0.73  0.21  0.92  0.73  0.66\n",
      "384  0.54  0.15  0.13  0.05  0.99  0.33  0.23  0.43  0.34  0.81\n",
      "385  0.52  0.13  0.70  0.21  0.54  0.45  0.29  0.50  0.78  0.78\n",
      "386  0.02  0.59  0.69  0.99  0.25  0.23  0.22  0.20  0.06  0.82\n",
      "387  0.96  0.83  0.17  0.65  0.69  0.36  0.90  0.30  0.77  0.68\n",
      "388  0.88  0.07  0.02  0.50  0.94  0.63  0.18  0.26  0.42  0.59\n",
      "389  0.18  0.36  0.07  0.23  0.32  0.19  0.99  0.53  0.81  0.97\n",
      "390  0.66  0.37  0.51  0.48  0.94  0.16  0.36  0.40  0.34  0.98\n",
      "391  0.62  0.62  0.78  0.06  0.13  0.46  0.71  0.00  0.95  0.72\n",
      "392  0.15  0.92  0.87  0.35  0.91  0.69  0.31  0.06  0.69  0.52\n",
      "393  0.62  0.25  0.92  0.63  0.54  0.32  1.00  0.89  0.46  0.52\n",
      "394  0.12  1.00  0.88  0.60  0.80  0.37  0.94  0.74  0.97  0.60\n",
      "395  0.23  0.54  0.16  0.03  0.92  0.73  0.16  0.20  0.44  0.09\n",
      "396  0.94  0.15  0.27  0.76  0.24  0.63  0.89  0.73  0.92  0.00\n",
      "397  0.69  0.52  0.08  0.29  0.78  0.68  0.39  0.86  0.97  0.11\n",
      "398  0.51  0.12  0.29  0.89  0.39  0.78  0.71  0.59  0.91  0.68\n",
      "399  0.19  0.90  0.86  0.08  0.93  0.13  0.31  0.70  0.39  0.86\n",
      "\n",
      "[400 rows x 10 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "*********************************\n",
      "          C_E   C_I  C_ID   C_C   C_T   C_R   C_A   C_P  C_ETB\n",
      "C_1_1    0.34  0.51  0.68  0.29  0.75  0.20  0.65  0.34   0.72\n",
      "C_1_2    0.81  0.28  0.34  0.72  0.96  0.34  0.68  0.27   0.78\n",
      "C_1_3    0.82  0.47  0.51  0.44  0.36  0.11  0.83  0.98   0.58\n",
      "C_1_4    0.49  0.37  0.24  0.47  0.49  0.44  0.72  0.01   0.65\n",
      "C_1_5    0.58  0.64  0.52  0.86  0.39  0.65  0.44  0.90   0.27\n",
      "C_2_1    0.82  0.83  0.58  0.14  0.48  0.97  0.85  0.78   0.77\n",
      "C_2_2    0.13  0.23  0.17  0.98  0.20  0.39  0.75  0.52   0.50\n",
      "C_2_3    0.98  0.56  0.02  0.21  0.83  0.92  0.05  0.14   0.62\n",
      "C_2_4    0.69  0.31  0.87  0.98  0.97  0.06  0.68  0.82   0.15\n",
      "C_2_5    0.07  0.30  0.21  0.78  0.26  0.64  0.02  0.07   0.22\n",
      "C_2_6    0.80  0.12  0.29  0.86  0.08  0.47  0.80  0.38   0.49\n",
      "C_2_7    0.38  0.27  0.11  0.76  0.59  0.00  0.60  0.89   0.63\n",
      "C_3_1    0.35  0.48  0.58  0.95  0.97  0.87  0.19  0.81   0.20\n",
      "C_3_2    0.61  0.34  0.66  0.94  0.66  0.58  0.58  0.14   0.56\n",
      "C_3_3    0.85  0.87  0.48  0.22  0.91  0.47  0.18  0.12   0.11\n",
      "C_3_4    0.46  0.79  0.78  0.34  0.04  0.87  0.58  0.16   0.87\n",
      "C_3_5    0.67  0.94  0.24  0.25  0.39  0.94  0.61  0.22   0.73\n",
      "C_3_6    0.89  0.61  0.02  0.25  0.48  0.52  0.12  0.59   0.76\n",
      "C_4_1    0.39  0.99  0.91  0.10  0.05  0.16  0.24  0.24   0.20\n",
      "C_4_2    0.57  0.15  0.56  0.25  0.76  0.88  0.54  0.42   0.91\n",
      "C_4_3    0.85  0.41  0.37  0.32  0.67  0.49  0.57  0.20   0.18\n",
      "C_4_4    0.54  0.58  0.25  0.67  0.61  0.29  0.14  0.03   0.77\n",
      "C_4_5    0.24  0.06  0.48  0.04  0.50  0.00  0.99  0.18   0.29\n",
      "C_4_6    0.53  0.46  0.22  0.77  0.88  0.92  0.45  0.06   0.25\n",
      "C_4_7    0.43  0.04  0.20  0.90  0.16  0.15  0.17  0.08   0.91\n",
      "C_5_1    0.96  0.99  0.04  0.46  0.30  0.04  0.73  0.99   0.75\n",
      "C_5_2    0.76  0.82  0.47  0.99  0.39  0.29  0.90  0.21   0.32\n",
      "C_5_3    0.36  0.26  0.07  0.77  0.68  0.73  0.21  0.93   0.09\n",
      "C_5_4    0.74  0.20  0.09  0.23  0.39  0.79  0.68  0.08   0.96\n",
      "C_5_5    0.37  0.06  0.80  0.10  0.84  0.82  0.04  0.30   0.15\n",
      "...       ...   ...   ...   ...   ...   ...   ...   ...    ...\n",
      "C_96_7   0.83  0.68  0.40  0.14  0.65  0.11  1.00  0.79   0.45\n",
      "C_96_8   0.82  0.44  0.78  0.78  0.48  0.99  0.01  0.08   0.10\n",
      "C_96_9   0.10  0.82  0.42  0.54  0.94  0.35  0.57  0.55   0.46\n",
      "C_96_10  0.83  0.23  0.37  0.36  0.53  0.60  0.74  0.42   0.58\n",
      "C_97_1   0.47  0.88  0.85  0.31  0.78  0.90  0.79  0.91   0.56\n",
      "C_97_2   0.27  0.99  0.82  0.18  0.64  0.17  0.20  0.03   0.44\n",
      "C_97_3   0.52  0.24  0.92  0.60  0.55  0.56  0.30  0.90   0.87\n",
      "C_97_4   0.72  0.63  0.19  0.73  0.51  0.06  0.20  0.42   0.47\n",
      "C_97_5   0.11  0.75  0.61  0.36  0.34  0.95  0.92  0.38   0.89\n",
      "C_98_1   0.20  0.90  0.05  0.09  0.21  0.94  0.51  0.09   0.99\n",
      "C_98_2   0.78  0.94  0.14  0.23  0.57  0.58  0.50  0.23   0.90\n",
      "C_98_3   0.74  0.44  0.03  0.64  0.73  0.50  0.02  0.54   0.43\n",
      "C_98_4   0.75  0.32  0.98  0.30  0.90  0.86  0.21  0.76   0.94\n",
      "C_98_5   0.06  0.53  0.76  0.66  0.03  0.91  0.37  0.95   0.81\n",
      "C_98_6   0.47  0.68  0.84  0.28  0.51  0.20  0.95  0.19   0.52\n",
      "C_99_1   0.48  0.69  0.63  0.96  0.77  0.86  0.18  0.02   0.17\n",
      "C_99_2   0.77  0.89  0.75  0.92  0.68  0.51  0.55  0.56   0.94\n",
      "C_99_3   0.88  0.37  0.02  0.37  0.79  0.89  0.33  0.98   0.00\n",
      "C_99_4   0.12  0.44  0.76  0.90  0.48  0.55  0.06  0.22   0.54\n",
      "C_99_5   0.94  0.20  0.27  0.24  0.77  0.35  0.52  0.80   0.58\n",
      "C_99_6   0.04  0.62  0.94  0.03  0.48  0.19  0.83  0.15   0.51\n",
      "C_99_7   0.59  0.80  0.45  0.74  0.90  0.22  0.83  0.99   0.63\n",
      "C_99_8   0.75  0.13  0.51  0.75  0.69  0.10  0.57  0.27   0.79\n",
      "C_99_9   0.21  0.10  0.68  0.17  0.13  0.79  0.48  0.56   0.42\n",
      "C_100_1  0.48  0.17  0.34  0.41  0.75  0.32  0.15  0.94   0.64\n",
      "C_100_2  0.91  0.76  0.17  0.14  0.50  0.63  0.94  0.94   0.56\n",
      "C_100_3  0.67  0.04  0.26  0.46  0.94  0.30  0.43  0.04   0.28\n",
      "C_100_4  0.51  0.74  0.55  1.00  0.82  0.80  0.47  0.93   0.52\n",
      "C_100_5  0.80  0.40  0.53  0.55  0.91  0.80  0.13  0.20   0.79\n",
      "C_100_6  0.79  0.67  0.64  0.61  0.11  0.20  0.23  0.66   0.15\n",
      "\n",
      "[720 rows x 9 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "*********************************\n",
      "['T_1', 'T_2', 'T_3', 'T_4', 'T_5', 'T_6', 'T_7', 'T_8', 'T_9', 'T_10', 'T_11', 'T_12', 'T_13', 'T_14', 'T_15', 'T_16', 'T_17', 'T_18', 'T_19', 'T_20', 'T_21', 'T_22', 'T_23', 'T_24', 'T_25', 'T_26', 'T_27', 'T_28', 'T_29', 'T_30', 'T_31', 'T_32', 'T_33', 'T_34', 'T_35', 'T_36', 'T_37', 'T_38', 'T_39', 'T_40', 'T_41', 'T_42', 'T_43', 'T_44', 'T_45', 'T_46', 'T_47', 'T_48', 'T_49', 'T_50', 'T_51', 'T_52', 'T_53', 'T_54', 'T_55', 'T_56', 'T_57', 'T_58', 'T_59', 'T_60', 'T_61', 'T_62', 'T_63', 'T_64', 'T_65', 'T_66', 'T_67', 'T_68', 'T_69', 'T_70', 'T_71', 'T_72', 'T_73', 'T_74', 'T_75', 'T_76', 'T_77', 'T_78', 'T_79', 'T_80', 'T_81', 'T_82', 'T_83', 'T_84', 'T_85', 'T_86', 'T_87', 'T_88', 'T_89', 'T_90', 'T_91', 'T_92', 'T_93', 'T_94', 'T_95', 'T_96', 'T_97', 'T_98', 'T_99', 'T_100']\n",
      "<class 'list'>\n",
      "*********************************\n",
      "{'T_57': ['C_57_1', 'C_57_2', 'C_57_3', 'C_57_4', 'C_57_5', 'C_57_6', 'C_57_7', 'C_57_8', 'C_57_9', 'C_57_10'], 'T_82': ['C_82_1', 'C_82_2', 'C_82_3', 'C_82_4', 'C_82_5', 'C_82_6', 'C_82_7'], 'T_76': ['C_76_1', 'C_76_2', 'C_76_3', 'C_76_4', 'C_76_5', 'C_76_6', 'C_76_7', 'C_76_8', 'C_76_9'], 'T_83': ['C_83_1', 'C_83_2', 'C_83_3', 'C_83_4', 'C_83_5'], 'T_40': ['C_40_1', 'C_40_2', 'C_40_3', 'C_40_4', 'C_40_5', 'C_40_6', 'C_40_7'], 'T_75': ['C_75_1', 'C_75_2', 'C_75_3', 'C_75_4', 'C_75_5', 'C_75_6', 'C_75_7', 'C_75_8', 'C_75_9', 'C_75_10'], 'T_1': ['C_1_1', 'C_1_2', 'C_1_3', 'C_1_4', 'C_1_5'], 'T_46': ['C_46_1', 'C_46_2', 'C_46_3', 'C_46_4', 'C_46_5'], 'T_42': ['C_42_1', 'C_42_2', 'C_42_3', 'C_42_4', 'C_42_5', 'C_42_6', 'C_42_7', 'C_42_8', 'C_42_9'], 'T_36': ['C_36_1', 'C_36_2', 'C_36_3', 'C_36_4', 'C_36_5', 'C_36_6', 'C_36_7'], 'T_5': ['C_5_1', 'C_5_2', 'C_5_3', 'C_5_4', 'C_5_5', 'C_5_6', 'C_5_7'], 'T_60': ['C_60_1', 'C_60_2', 'C_60_3', 'C_60_4', 'C_60_5', 'C_60_6', 'C_60_7'], 'T_33': ['C_33_1', 'C_33_2', 'C_33_3', 'C_33_4', 'C_33_5', 'C_33_6'], 'T_67': ['C_67_1', 'C_67_2', 'C_67_3', 'C_67_4', 'C_67_5'], 'T_55': ['C_55_1', 'C_55_2', 'C_55_3', 'C_55_4', 'C_55_5', 'C_55_6', 'C_55_7', 'C_55_8', 'C_55_9'], 'T_85': ['C_85_1', 'C_85_2', 'C_85_3', 'C_85_4', 'C_85_5'], 'T_62': ['C_62_1', 'C_62_2', 'C_62_3', 'C_62_4', 'C_62_5', 'C_62_6'], 'T_77': ['C_77_1', 'C_77_2', 'C_77_3', 'C_77_4', 'C_77_5', 'C_77_6', 'C_77_7', 'C_77_8', 'C_77_9'], 'T_98': ['C_98_1', 'C_98_2', 'C_98_3', 'C_98_4', 'C_98_5', 'C_98_6'], 'T_91': ['C_91_1', 'C_91_2', 'C_91_3', 'C_91_4', 'C_91_5'], 'T_100': ['C_100_1', 'C_100_2', 'C_100_3', 'C_100_4', 'C_100_5', 'C_100_6'], 'T_65': ['C_65_1', 'C_65_2', 'C_65_3', 'C_65_4', 'C_65_5'], 'T_64': ['C_64_1', 'C_64_2', 'C_64_3', 'C_64_4', 'C_64_5', 'C_64_6', 'C_64_7', 'C_64_8'], 'T_80': ['C_80_1', 'C_80_2', 'C_80_3', 'C_80_4', 'C_80_5', 'C_80_6'], 'T_48': ['C_48_1', 'C_48_2', 'C_48_3', 'C_48_4', 'C_48_5', 'C_48_6', 'C_48_7'], 'T_54': ['C_54_1', 'C_54_2', 'C_54_3', 'C_54_4', 'C_54_5'], 'T_37': ['C_37_1', 'C_37_2', 'C_37_3', 'C_37_4', 'C_37_5', 'C_37_6', 'C_37_7', 'C_37_8', 'C_37_9'], 'T_89': ['C_89_1', 'C_89_2', 'C_89_3', 'C_89_4', 'C_89_5'], 'T_68': ['C_68_1', 'C_68_2', 'C_68_3', 'C_68_4', 'C_68_5', 'C_68_6'], 'T_44': ['C_44_1', 'C_44_2', 'C_44_3', 'C_44_4', 'C_44_5', 'C_44_6', 'C_44_7', 'C_44_8'], 'T_15': ['C_15_1', 'C_15_2', 'C_15_3', 'C_15_4', 'C_15_5', 'C_15_6', 'C_15_7', 'C_15_8', 'C_15_9'], 'T_79': ['C_79_1', 'C_79_2', 'C_79_3', 'C_79_4', 'C_79_5', 'C_79_6', 'C_79_7', 'C_79_8'], 'T_20': ['C_20_1', 'C_20_2', 'C_20_3', 'C_20_4', 'C_20_5', 'C_20_6', 'C_20_7', 'C_20_8', 'C_20_9', 'C_20_10'], 'T_16': ['C_16_1', 'C_16_2', 'C_16_3', 'C_16_4', 'C_16_5', 'C_16_6', 'C_16_7'], 'T_24': ['C_24_1', 'C_24_2', 'C_24_3', 'C_24_4', 'C_24_5', 'C_24_6'], 'T_47': ['C_47_1', 'C_47_2', 'C_47_3', 'C_47_4', 'C_47_5', 'C_47_6'], 'T_73': ['C_73_1', 'C_73_2', 'C_73_3', 'C_73_4', 'C_73_5', 'C_73_6', 'C_73_7', 'C_73_8'], 'T_50': ['C_50_1', 'C_50_2', 'C_50_3', 'C_50_4', 'C_50_5', 'C_50_6', 'C_50_7', 'C_50_8', 'C_50_9', 'C_50_10'], 'T_2': ['C_2_1', 'C_2_2', 'C_2_3', 'C_2_4', 'C_2_5', 'C_2_6', 'C_2_7'], 'T_22': ['C_22_1', 'C_22_2', 'C_22_3', 'C_22_4', 'C_22_5', 'C_22_6'], 'T_11': ['C_11_1', 'C_11_2', 'C_11_3', 'C_11_4', 'C_11_5', 'C_11_6', 'C_11_7', 'C_11_8', 'C_11_9'], 'T_74': ['C_74_1', 'C_74_2', 'C_74_3', 'C_74_4', 'C_74_5'], 'T_41': ['C_41_1', 'C_41_2', 'C_41_3', 'C_41_4', 'C_41_5', 'C_41_6', 'C_41_7'], 'T_25': ['C_25_1', 'C_25_2', 'C_25_3', 'C_25_4', 'C_25_5', 'C_25_6'], 'T_17': ['C_17_1', 'C_17_2', 'C_17_3', 'C_17_4', 'C_17_5', 'C_17_6', 'C_17_7'], 'T_96': ['C_96_1', 'C_96_2', 'C_96_3', 'C_96_4', 'C_96_5', 'C_96_6', 'C_96_7', 'C_96_8', 'C_96_9', 'C_96_10'], 'T_92': ['C_92_1', 'C_92_2', 'C_92_3', 'C_92_4', 'C_92_5', 'C_92_6', 'C_92_7', 'C_92_8', 'C_92_9'], 'T_32': ['C_32_1', 'C_32_2', 'C_32_3', 'C_32_4', 'C_32_5'], 'T_90': ['C_90_1', 'C_90_2', 'C_90_3', 'C_90_4', 'C_90_5', 'C_90_6', 'C_90_7'], 'T_58': ['C_58_1', 'C_58_2', 'C_58_3', 'C_58_4', 'C_58_5', 'C_58_6'], 'T_9': ['C_9_1', 'C_9_2', 'C_9_3', 'C_9_4', 'C_9_5', 'C_9_6', 'C_9_7', 'C_9_8', 'C_9_9'], 'T_4': ['C_4_1', 'C_4_2', 'C_4_3', 'C_4_4', 'C_4_5', 'C_4_6', 'C_4_7'], 'T_71': ['C_71_1', 'C_71_2', 'C_71_3', 'C_71_4', 'C_71_5', 'C_71_6'], 'T_28': ['C_28_1', 'C_28_2', 'C_28_3', 'C_28_4', 'C_28_5', 'C_28_6', 'C_28_7', 'C_28_8', 'C_28_9'], 'T_45': ['C_45_1', 'C_45_2', 'C_45_3', 'C_45_4', 'C_45_5', 'C_45_6', 'C_45_7', 'C_45_8'], 'T_94': ['C_94_1', 'C_94_2', 'C_94_3', 'C_94_4', 'C_94_5', 'C_94_6', 'C_94_7', 'C_94_8', 'C_94_9'], 'T_95': ['C_95_1', 'C_95_2', 'C_95_3', 'C_95_4', 'C_95_5', 'C_95_6'], 'T_81': ['C_81_1', 'C_81_2', 'C_81_3', 'C_81_4', 'C_81_5', 'C_81_6', 'C_81_7', 'C_81_8'], 'T_6': ['C_6_1', 'C_6_2', 'C_6_3', 'C_6_4', 'C_6_5', 'C_6_6', 'C_6_7'], 'T_49': ['C_49_1', 'C_49_2', 'C_49_3', 'C_49_4', 'C_49_5', 'C_49_6'], 'T_12': ['C_12_1', 'C_12_2', 'C_12_3', 'C_12_4', 'C_12_5'], 'T_23': ['C_23_1', 'C_23_2', 'C_23_3', 'C_23_4', 'C_23_5', 'C_23_6', 'C_23_7', 'C_23_8', 'C_23_9'], 'T_14': ['C_14_1', 'C_14_2', 'C_14_3', 'C_14_4', 'C_14_5', 'C_14_6', 'C_14_7'], 'T_72': ['C_72_1', 'C_72_2', 'C_72_3', 'C_72_4', 'C_72_5', 'C_72_6', 'C_72_7'], 'T_10': ['C_10_1', 'C_10_2', 'C_10_3', 'C_10_4', 'C_10_5', 'C_10_6', 'C_10_7', 'C_10_8', 'C_10_9', 'C_10_10'], 'T_59': ['C_59_1', 'C_59_2', 'C_59_3', 'C_59_4', 'C_59_5', 'C_59_6'], 'T_34': ['C_34_1', 'C_34_2', 'C_34_3', 'C_34_4', 'C_34_5', 'C_34_6', 'C_34_7', 'C_34_8'], 'T_13': ['C_13_1', 'C_13_2', 'C_13_3', 'C_13_4', 'C_13_5', 'C_13_6', 'C_13_7', 'C_13_8', 'C_13_9'], 'T_70': ['C_70_1', 'C_70_2', 'C_70_3', 'C_70_4', 'C_70_5', 'C_70_6'], 'T_39': ['C_39_1', 'C_39_2', 'C_39_3', 'C_39_4', 'C_39_5', 'C_39_6'], 'T_27': ['C_27_1', 'C_27_2', 'C_27_3', 'C_27_4', 'C_27_5', 'C_27_6', 'C_27_7', 'C_27_8', 'C_27_9'], 'T_21': ['C_21_1', 'C_21_2', 'C_21_3', 'C_21_4', 'C_21_5', 'C_21_6', 'C_21_7', 'C_21_8'], 'T_3': ['C_3_1', 'C_3_2', 'C_3_3', 'C_3_4', 'C_3_5', 'C_3_6'], 'T_19': ['C_19_1', 'C_19_2', 'C_19_3', 'C_19_4', 'C_19_5', 'C_19_6', 'C_19_7', 'C_19_8', 'C_19_9', 'C_19_10'], 'T_63': ['C_63_1', 'C_63_2', 'C_63_3', 'C_63_4', 'C_63_5', 'C_63_6', 'C_63_7', 'C_63_8'], 'T_78': ['C_78_1', 'C_78_2', 'C_78_3', 'C_78_4', 'C_78_5'], 'T_26': ['C_26_1', 'C_26_2', 'C_26_3', 'C_26_4', 'C_26_5', 'C_26_6'], 'T_61': ['C_61_1', 'C_61_2', 'C_61_3', 'C_61_4', 'C_61_5', 'C_61_6', 'C_61_7', 'C_61_8', 'C_61_9'], 'T_56': ['C_56_1', 'C_56_2', 'C_56_3', 'C_56_4', 'C_56_5', 'C_56_6'], 'T_88': ['C_88_1', 'C_88_2', 'C_88_3', 'C_88_4', 'C_88_5', 'C_88_6'], 'T_7': ['C_7_1', 'C_7_2', 'C_7_3', 'C_7_4', 'C_7_5', 'C_7_6', 'C_7_7', 'C_7_8', 'C_7_9', 'C_7_10'], 'T_99': ['C_99_1', 'C_99_2', 'C_99_3', 'C_99_4', 'C_99_5', 'C_99_6', 'C_99_7', 'C_99_8', 'C_99_9'], 'T_84': ['C_84_1', 'C_84_2', 'C_84_3', 'C_84_4', 'C_84_5', 'C_84_6', 'C_84_7', 'C_84_8'], 'T_97': ['C_97_1', 'C_97_2', 'C_97_3', 'C_97_4', 'C_97_5'], 'T_43': ['C_43_1', 'C_43_2', 'C_43_3', 'C_43_4', 'C_43_5', 'C_43_6', 'C_43_7', 'C_43_8'], 'T_87': ['C_87_1', 'C_87_2', 'C_87_3', 'C_87_4', 'C_87_5', 'C_87_6', 'C_87_7'], 'T_30': ['C_30_1', 'C_30_2', 'C_30_3', 'C_30_4', 'C_30_5', 'C_30_6', 'C_30_7', 'C_30_8'], 'T_52': ['C_52_1', 'C_52_2', 'C_52_3', 'C_52_4', 'C_52_5', 'C_52_6', 'C_52_7', 'C_52_8', 'C_52_9'], 'T_31': ['C_31_1', 'C_31_2', 'C_31_3', 'C_31_4', 'C_31_5'], 'T_51': ['C_51_1', 'C_51_2', 'C_51_3', 'C_51_4', 'C_51_5'], 'T_66': ['C_66_1', 'C_66_2', 'C_66_3', 'C_66_4', 'C_66_5', 'C_66_6', 'C_66_7'], 'T_93': ['C_93_1', 'C_93_2', 'C_93_3', 'C_93_4', 'C_93_5', 'C_93_6', 'C_93_7', 'C_93_8', 'C_93_9', 'C_93_10'], 'T_35': ['C_35_1', 'C_35_2', 'C_35_3', 'C_35_4', 'C_35_5', 'C_35_6', 'C_35_7'], 'T_86': ['C_86_1', 'C_86_2', 'C_86_3', 'C_86_4', 'C_86_5', 'C_86_6', 'C_86_7'], 'T_53': ['C_53_1', 'C_53_2', 'C_53_3', 'C_53_4', 'C_53_5', 'C_53_6', 'C_53_7'], 'T_29': ['C_29_1', 'C_29_2', 'C_29_3', 'C_29_4', 'C_29_5', 'C_29_6', 'C_29_7'], 'T_69': ['C_69_1', 'C_69_2', 'C_69_3', 'C_69_4', 'C_69_5'], 'T_8': ['C_8_1', 'C_8_2', 'C_8_3', 'C_8_4', 'C_8_5'], 'T_38': ['C_38_1', 'C_38_2', 'C_38_3', 'C_38_4', 'C_38_5', 'C_38_6', 'C_38_7', 'C_38_8', 'C_38_9'], 'T_18': ['C_18_1', 'C_18_2', 'C_18_3', 'C_18_4', 'C_18_5', 'C_18_6', 'C_18_7', 'C_18_8', 'C_18_9']}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import os,pickle\n",
    "\n",
    "# file_path = os.path.join(os.path.curdir, '..' , 'dataset' , 'very_small')\n",
    "# file_path = os.path.join(os.path.curdir, '..' , 'dataset' , 'small')\n",
    "# file_path = os.path.join(os.path.curdir, '..' , 'dataset' , 'medium')\n",
    "file_path = os.path.join(os.path.curdir, '..' , 'dataset' , 'large')\n",
    "# file_path = os.path.join(os.path.curdir, '..' , 'dataset' , 'very_large')\n",
    "'''\n",
    "Context data for learning\n",
    "'''\n",
    "class Context:\n",
    "    \"\"\"\n",
    "    Contextual information required by contextual bandit algorithms to make better predictions. It enscapsulates all data\n",
    "    about the student , topics & content to experiment with the learning algorithm. \n",
    "    \"\"\"\n",
    "    \n",
    "    def getStudentContext(self):\n",
    "        \"\"\"\n",
    "        Student Preferences: \n",
    "        Visual (S_V) , Text (S_T) , Demo-based (S_D) , Practical (S_P), Step-by-step (S_S) ,Activity / Task based (S_AT), \n",
    "        Lecture (S_L) , Audio (S_A) , Self-evaluation (S_SE) , Pre-assessment (S_PA)\n",
    "        Students preference to learning via various ways are evaluated on a scale from 0 to 1, rather being binary. \n",
    "        \"\"\"\n",
    "        return self.studentContext\n",
    "    \n",
    "    def setStudentContext(self):\n",
    "        \"\"\"\n",
    "        Load the student data\n",
    "        \"\"\"\n",
    "        with open(os.path.join(file_path , 'student.pickle'), 'rb') as student_file:\n",
    "            self.studentContext= pickle.load(student_file)\n",
    "    \n",
    "    \n",
    "    def getContentContext(self):\n",
    "        \"\"\"\n",
    "        Content Features \n",
    "        Ease of understanding (C_E) , Simple / Intuitive (C_I) , Surface / In-depth (C_ID) , Brief / Concise (C_C), \n",
    "        Thorough (C_T), Preference / Well reviewed / Well rated (C_R) , Theoritical / Abstract (C_A), \n",
    "        Practical / Hands on (C_P), Experimental / Task-based (C_ETB)\n",
    "        Content preference to learning via various ways are evaluated on a scale from 0 to 1, rather being binary. \n",
    "        \"\"\"\n",
    "        return self.contentContext\n",
    "   \n",
    "    def setContentContext(self):\n",
    "        \"\"\"\n",
    "        Load the content data\n",
    "        \"\"\"\n",
    "        with open(os.path.join(file_path ,'content.pickle'), 'rb') as content_file:\n",
    "            self.contentContext= pickle.load(content_file)\n",
    "        \n",
    "    def getTopic(self):\n",
    "        \"\"\"\n",
    "        Gives the topics part of the course.\n",
    "        \"\"\"\n",
    "        return self.topic\n",
    "    \n",
    "    def setTopic(self):\n",
    "        \"\"\"\n",
    "        Loads the topics part of the course\n",
    "        \"\"\"\n",
    "        with open(os.path.join(file_path ,'topic.pickle'), 'rb') as topic_file:\n",
    "            self.topic = pickle.load(topic_file)\n",
    "    \n",
    "    def getTopicContent(self):\n",
    "        \"\"\"\n",
    "         Gets the topic content. topic_content is a map of topics to content. So for every topic, it gives the content \n",
    "         available for the topic. In education parlance, for any given topic, it shows the different ways of teaching this\n",
    "         topic (via contents)\n",
    "        \"\"\"\n",
    "        return self.topic_content\n",
    "    \n",
    "    def setTopicContent(self):\n",
    "        \"\"\"\n",
    "        Sets the topic_content variable to the one in the serialized object. topic_content is a map of topics to content. So\n",
    "        for every topic, it gives the content available for the topic. In education parlance, for any given topic, it shows\n",
    "        the different ways of teaching this topic (via contents)\n",
    "        \"\"\"\n",
    "        with open(os.path.join(file_path ,'topic_content.pickle'), 'rb') as topic_content_file:\n",
    "            self.topic_content= pickle.load(topic_content_file)\n",
    "                \n",
    "    def prepareContext(self,studentContext,contentContext):\n",
    "        \"\"\"\n",
    "           Given the student & content context available for a round, this method combines them to form a single contextual\n",
    "           variable\n",
    "           \n",
    "           Inputs : \n",
    "           \n",
    "           studentContext: Student contextual information.\n",
    "           contentContext: Contents contextual information. \n",
    "           \n",
    "           Returns :\n",
    "           \n",
    "           context : A combined output of student & content context.\n",
    "        \"\"\"\n",
    "        context = pd.DataFrame() \n",
    "        for content in list(contentContext.index):\n",
    "            c = pd.Series()\n",
    "            c = c.append([studentContext,contentContext.loc[content]]) # Combine student & content. \n",
    "            c['Content_id'] = content\n",
    "            context = context.append(c, ignore_index=True)\n",
    "        context = context.set_index('Content_id')\n",
    "        return context\n",
    "    \n",
    "    \n",
    "    def loadData(self):\n",
    "        \"\"\"\n",
    "        Method used to test data retrieval. Data generator handles the data generation. This method checks we can retrieve\n",
    "        data. This is a dummy method used to test data retrieval. Its not invoked in the main program.\n",
    "        \"\"\"\n",
    "        self.setStudentContext()\n",
    "        self.setContentContext()\n",
    "        self.setTopic()\n",
    "        self.setTopicContent()\n",
    "        print(self.getStudentContext())\n",
    "        print(type(self.getStudentContext()))\n",
    "        print('*********************************')\n",
    "        print(self.getContentContext())\n",
    "        print(type(self.getContentContext()))\n",
    "        print('*********************************')\n",
    "        print(self.getTopic())\n",
    "        print(type(self.getTopic()))\n",
    "        print('*********************************')\n",
    "        print(self.getTopicContent())\n",
    "        print(type(self.getTopicContent()))\n",
    "                \n",
    "c_test = Context()\n",
    "c_test.loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online Stochastic Gradient Descent. This classifier decides whether or not to skip to the next topic. \n",
    "# TO-DO : Change loss functions (Log,Hinge,Others) to find if they impact performance. Try different values of parameters \n",
    "# For instance SGD has a parameter alpha, SVM has a parameter C. To optimize, you can train a mini-batch of samples, \n",
    "# rather than one data point at a time. Try different values of learning_rate . Look at the class_weight parameter if you \n",
    "# want to give more weight to samples of one class over the other. Need to understand about warm_start parameter\n",
    "# We need to record predictions made by the classifier to evaluate its performance over rounds \n",
    "from sklearn import linear_model\n",
    "import os.path\n",
    "import threading\n",
    "class SkipClassifier:\n",
    "    \"\"\"\n",
    "    A classifier which gives prediction, whether or not to move to the next topic. This is important, because we want \n",
    "    students to learn content which the algorithm is confident would help the student learn. The skip classifier is trained\n",
    "    online, hence we use a confidence threshold, to be conservative & minimize skipping topics. Skipping is not preferred, \n",
    "    but if the classifier is confident the next round would help gain higher rewards, then we should skip. Ideally, we want \n",
    "    to consider skipping after the first pulled arm has failed, to avoid frustrating the student. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.clf = linear_model.SGDClassifier()\n",
    "        self.clf.partial_fit(np.array([[0,0,0,0,0,0,0,0,0,0,0,0]]),np.array([0]),classes=np.array([0,1])) # Used to initialize the skip classifier\n",
    "#         self.classifier_lock = threading.Lock()\n",
    "#         if os.path.exists('skip_classifier_linUCB.sav'):\n",
    "#             self.clf = pickle.load(open('skip_classifier_linUCB.sav', 'rb'))\n",
    "#         else:\n",
    "#             self.clf = linear_model.SGDClassifier()\n",
    "#             self.clf.partial_fit(np.array([[0,0,0,0,0,0,0,0,0,0,0,0]]),np.array([0]),classes=np.array([0,1])) # Used to initialize the skip classifier\n",
    "#         self.classifier_lock = threading.Lock()\n",
    "        \n",
    "    def check_fitted(self,clf): \n",
    "        \"\"\"\n",
    "        Check if the classifier is fit before asking for prediction. Our classifier is trained in online mode, hence it would\n",
    "        be asked to predict before fitting. This method makes sure we only ask for prediction after a data point has been \n",
    "        fit to the estimator/model\n",
    "        \"\"\"\n",
    "        return hasattr(clf, \"classes_\")\n",
    "    \n",
    "    def train(self,student,pta,next_topic_pta,label):\n",
    "        \"\"\"\n",
    "        Used to train the classifier in online mode, over every data point. In future we might want to consider training in \n",
    "        mini-batches, rather than for every data point. \n",
    "        \"\"\"\n",
    "        X = pd.Series()\n",
    "        X = X.append([student,pd.Series([pta,next_topic_pta],index=['pta','next_topic_pta'])])\n",
    "        X = np.array([X.values])\n",
    "        Y = np.array([label])\n",
    "        self.clf = self.clf.partial_fit(X,Y,classes=np.array([0,1]))\n",
    "        pickle.dump(self.clf, open('skip_classifier_linUCB_large.sav', 'wb'))\n",
    "            \n",
    "    def predict(self,student,pta,next_topic_pta):\n",
    "        \"\"\"\n",
    "        Gets predictions from the classifier, along with the confidence score to help determine the reliability / confidence\n",
    "        level of the prediction being made. \n",
    "        \"\"\"\n",
    "#         with self.classifier_lock:\n",
    "#             print('pta : {0} , next_topic_pta : {1}'.format(pta,next_topic_pta))\n",
    "        X = pd.Series()\n",
    "        X = X.append([student,pd.Series([pta,next_topic_pta],index=['pta','next_topic_pta'])])\n",
    "        if self.check_fitted(self.clf):\n",
    "            Y = self.clf.predict([X.values])[0]\n",
    "            confidence_score = self.clf.decision_function([X.values])[0]\n",
    "        else:\n",
    "            Y = 0\n",
    "            confidence_score = 0\n",
    "        return int(Y) , confidence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipTopic:\n",
    "    \"\"\"\n",
    "    A wrapper around the Skip Classifier to validate the inputs, before sending it to Skip Classifier for prediction. \n",
    "    It post-processes the results of the prediction made by skip classifier to check for confidence threshold, \n",
    "    before sending out the decision to skip or not. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SkipTopic class & sets confidence threshold to make confident skip decisions.\n",
    "        \"\"\"\n",
    "        self.skipClassifier = SkipClassifier()\n",
    "        self.confidence_threshold = 100 # It the confidence score returned by the classifier is greater than this, then we trust in the decision made by the classifier. \n",
    "        self.threshold_updated_count = 1 \n",
    "        self.skip_topic_lock = threading.Lock()\n",
    "                \n",
    "    def skipTopic(self,student,pta,topic_number,context_obj,topic_content,linUCB):\n",
    "        \"\"\"\n",
    "        Pre-validates the topic number before asking the skip classifier for a prediction. Then checks the confidence \n",
    "        of the prediction before sending out the decision to skip or not. \n",
    "        \"\"\"\n",
    "        contentContext = context_obj.getContentContext() # Get the content dataframe.\n",
    "        topic = context_obj.getTopic() # Get the topic list. \n",
    "        current_topic_index = topic.index(topic_number) # Get the index number of the current topic\n",
    "        next_topic_index = current_topic_index + 1\n",
    "        next_topic = '' # Initialized to make it accessible outside the if statement. \n",
    "        if next_topic_index < len(topic): # Check to see if we're going out of bounds\n",
    "            next_topic = topic[next_topic_index]\n",
    "            next_topic_contents = topic_content[next_topic]\n",
    "            t_c = contentContext.loc[next_topic_contents]\n",
    "            X = context_obj.prepareContext(student,t_c)\n",
    "            arm_pulled , next_topic_pta = linUCB.expectedPayoff(X,next_topic_contents)\n",
    "        else:\n",
    "            # Will be going out of bounds. Current topic is the last topic. No more topics to complete. \n",
    "            next_topic_pta = 0\n",
    "        actual_decision , confidence_score = self.skipClassifier.predict(student,pta,next_topic_pta)\n",
    "        if actual_decision and confidence_score > self.confidence_threshold:\n",
    "            skip_decision = 1\n",
    "        else:\n",
    "            skip_decision = 0\n",
    "        return actual_decision,confidence_score,skip_decision,next_topic_pta\n",
    "        \n",
    "    def setLabel(self,actual_payoff):\n",
    "        \"\"\"\n",
    "        Sets the label for training the skip classifier\n",
    "        \"\"\"\n",
    "#         if actual_payoff == -1:\n",
    "        if actual_payoff == 0:\n",
    "            label = 1\n",
    "        if actual_payoff == 1:\n",
    "            label = 0\n",
    "        return label\n",
    "    \n",
    "    def train(self,student,pta,pta_next_topic,label):\n",
    "        \"\"\"\n",
    "        Training the skip classifier\n",
    "        \"\"\"\n",
    "        self.skipClassifier.train(student,pta,pta_next_topic,label)\n",
    "        \n",
    "    def updateConfidenceThreshold(self , rounds):\n",
    "        if np.log10(rounds) > self.threshold_updated_count : \n",
    "            self.confidence_threshold /= np.log10(rounds)\n",
    "            self.threshold_updated_count += 1\n",
    "            with self.skip_topic_lock:\n",
    "                print(\"self.confidence_threshold : {0} and self.threshold_updated_count : {1}\".format(self.confidence_threshold,self.threshold_updated_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature / Environment / Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle : \n",
    "    \"\"\"\n",
    "    It has the optimal parameters to maximize rewards. The learning algorithm updates its parameters to emulate its parameters\n",
    "    It is an omniscient policy that knows all the probability distributions. At every step of the way, makes the best \n",
    "    decision based on its knowledge of the true distributions. Iit does not have to learn anything. \n",
    "    \"\"\"\n",
    "    def setParameters(self, contexts , arms):\n",
    "        \"\"\"\n",
    "        Sets the optimal parameters for the omniscient policy. \n",
    "        \"\"\"\n",
    "        parameters = np.random.uniform(size=(len(arms) , len(contexts)))\n",
    "        # Normalize parameters\n",
    "        for i in range(parameters.shape[0]): # Have it in a list comprehension.\n",
    "            parameters[i] = parameters[i] / np.sum(parameters[i])\n",
    "        self.theta_df = pd.DataFrame(data = parameters ,  index = arms , columns = contexts , dtype= np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class LinUCB:\n",
    "    \"\"\"\n",
    "    The learning algorithm, which suggest the arm to be pulled / content to be shown. Based on students feedback it updates\n",
    "    the parameters of the pulled arm. We're using the disjoint linear model described in this paper[LinUCB](https://arxiv.org/abs/1003.0146) \n",
    "    This is the most cited contextual bandit algorithm. \n",
    "    \"\"\"\n",
    "#     def __init__(self,alpha=0.001):\n",
    "    def __init__(self,alpha=0.75):\n",
    "        \"\"\"\n",
    "        Initialize the variables \n",
    "        alpha : To adjust confidence bounds. Higher alpha, implies arms would have higher confidence bounds. This parameter \n",
    "        should be tuned to be optimal.\n",
    "        arm_params : A map , which for every arm stores the corresponding arm object used to update parameters of an arm.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha # Hyper parameter required for LinUCB to adjust confidence bounds.\n",
    "        self.arm_params = {} # Maps content to arm object\n",
    "#         self.LinUCB_lock = threading.Lock()\n",
    "                \n",
    "    def expectedPayoff(self,contexts,arms):\n",
    "        \"\"\"\n",
    "        Gives the max expected pay-off for a round with the given context & available arms. \n",
    "        \n",
    "        Input : \n",
    "        \n",
    "        contexts : Contextual data available in the round. Its a combination of student & content context\n",
    "        arms : Arms / Content available in this round. \n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "        arm_pulled : The arm that should be pulled \n",
    "        expected_payoff : Expected pay-off for the pulled suggested to be pulled. \n",
    "        \n",
    "        The arm is not pulled up here as we also depend of the decision from the skip classifer before the arm is actually pulled        \n",
    "        \"\"\"\n",
    "        arms_payoff = list()\n",
    "        for arm in arms:\n",
    "            X = contexts.loc[arm] # Give student & content context for an arm \n",
    "            if arm not in self.arm_params: # If new content is added, then parameters would be created for it. \n",
    "                self.arm_params[arm] = Arm(len(X.index)) # Arm class below, has arm specific parameters\n",
    "            arm_obj = self.arm_params[arm]\n",
    "            theta = self.getTheta(arm_obj) # Arm parameter. \n",
    "            pta = self.getPta(X , arm_obj) # pta : pay-off/reward at round 't' for arm 'a'. \n",
    "            arms_payoff.append(pta)\n",
    "            # Commenting the normalization code to help the skipping feature make better decisions. \n",
    "#         for i in range(len(arms_payoff)): # Normalize arms_payoff. Required for cases when alpha > 1. Have it in a list comprehension.\n",
    "#             arms_payoff[i] = arms_payoff[i] / np.sum(arms_payoff)\n",
    "        expected_payoff = np.max(arms_payoff) # To be used a input data for skip algorithm\n",
    "        arm_index = np.argmax(arms_payoff) # Find the index of the arm which max pay-off\n",
    "        arm_pulled = arms[arm_index] # Give me the arm with max pay-off \n",
    "        return arm_pulled,np.round(expected_payoff,2)    \n",
    "        \n",
    "    def getTheta(self,arm):  \n",
    "        \"\"\"\n",
    "        Get theta which is used to compute the mean reward for an arm\n",
    "        \"\"\"\n",
    "        arm.theta = np.dot(arm.Ainv , arm.b) # A vector\n",
    "        return arm.theta\n",
    "    \n",
    "    def getMean(self, context , arm):\n",
    "        \"\"\"\n",
    "        Get mean expected reward for an arm \n",
    "        \"\"\"\n",
    "        mean = np.dot(arm.theta.T , context)\n",
    "        return mean\n",
    "        \n",
    "    def getUCB(self , context ,arm):\n",
    "        \"\"\"\n",
    "        Get upper confidence bound for an arm \n",
    "        \"\"\"\n",
    "        ucb = np.sqrt(np.dot(np.dot(context.T , arm.Ainv) , context))\n",
    "        return ucb\n",
    "    \n",
    "    def getPta(self, context , arm):\n",
    "        \"\"\"\n",
    "        Get expected pay-off for an arm \n",
    "        \"\"\"\n",
    "        payoff = self.getMean(context,arm) + self.alpha * self.getUCB(context , arm)\n",
    "        return payoff\n",
    "    \n",
    "    def updateParams(self, arm , context, reward):\n",
    "        \"\"\"\n",
    "        Update parameters for the pulled arm. \n",
    "        \"\"\"\n",
    "        arm_obj = self.arm_params[arm]\n",
    "        arm_obj.A += np.outer(context,context.T)\n",
    "        arm_obj.b += reward * context\n",
    "        arm_obj.Ainv = np.linalg.inv(arm_obj.A)               \n",
    "    \n",
    "class Arm:\n",
    "    \"\"\"\n",
    "    Arm class which enscapulates arm parameters, which are updated for an arm when its pulled. \n",
    "    \"\"\"\n",
    "    def __init__(self,dimensions):\n",
    "        \"\"\"\n",
    "        Initialize the arm parameters. \n",
    "        \"\"\"\n",
    "        self.A = np.identity(dimensions)\n",
    "        self.b = np.zeros(dimensions)\n",
    "        self.Ainv = np.linalg.inv(self.A)\n",
    "        self.theta = np.dot(self.Ainv , self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pta values before they are sent for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rounds completed\n",
      "self.confidence_threshold : 85.02741537276025 and self.threshold_updated_count : 2\n",
      "self.confidence_threshold : 42.422046925684775 and self.threshold_updated_count : 3\n",
      "1002 rounds completed\n",
      "self.confidence_threshold : 14.136593433061568 and self.threshold_updated_count : 4\n",
      "2002 rounds completed\n",
      "3002 rounds completed\n",
      "4002 rounds completed\n",
      "5002 rounds completed\n",
      "6002 rounds completed\n",
      "7002 rounds completed\n",
      "8002 rounds completed\n",
      "9002 rounds completed\n",
      "self.confidence_threshold : 3.5341099890721526 and self.threshold_updated_count : 5\n",
      "10002 rounds completed\n",
      "11002 rounds completed\n",
      "12002 rounds completed\n",
      "13002 rounds completed\n",
      "14002 rounds completed\n",
      "15002 rounds completed\n",
      "16002 rounds completed\n",
      "17002 rounds completed\n",
      "18002 rounds completed\n",
      "19002 rounds completed\n",
      "20002 rounds completed\n",
      "21002 rounds completed\n",
      "22002 rounds completed\n",
      "23002 rounds completed\n",
      "24002 rounds completed\n",
      "25002 rounds completed\n",
      "26002 rounds completed\n",
      "27002 rounds completed\n",
      "28002 rounds completed\n",
      "29002 rounds completed\n",
      "30002 rounds completed\n",
      "31002 rounds completed\n",
      "32002 rounds completed\n",
      "33002 rounds completed\n",
      "34002 rounds completed\n",
      "35002 rounds completed\n",
      "36002 rounds completed\n",
      "37002 rounds completed\n",
      "38002 rounds completed\n",
      "39002 rounds completed\n",
      "40002 rounds completed\n",
      "41002 rounds completed\n",
      "42002 rounds completed\n",
      "43002 rounds completed\n",
      "44002 rounds completed\n",
      "45002 rounds completed\n",
      "46002 rounds completed\n",
      "47002 rounds completed\n",
      "48002 rounds completed\n",
      "49002 rounds completed\n",
      "50002 rounds completed\n",
      "51002 rounds completed\n",
      "52002 rounds completed\n",
      "53002 rounds completed\n",
      "54002 rounds completed\n",
      "55002 rounds completed\n",
      "56002 rounds completed\n",
      "57002 rounds completed\n",
      "58002 rounds completed\n",
      "59002 rounds completed\n",
      "60002 rounds completed\n",
      "61002 rounds completed\n",
      "62002 rounds completed\n",
      "63002 rounds completed\n",
      "64002 rounds completed\n",
      "65002 rounds completed\n",
      "66002 rounds completed\n",
      "67002 rounds completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7f2600465ae8>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap>\", line 191, in cb\n",
      "KeyError: 'pandas._libs.pandas.core.dtypes.cast'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68002 rounds completed\n",
      "69002 rounds completed\n",
      "70002 rounds completed\n",
      "71002 rounds completed\n",
      "72002 rounds completed\n",
      "73002 rounds completed\n",
      "74002 rounds completed\n",
      "Total Number of rounds :  74957\n"
     ]
    }
   ],
   "source": [
    "import threading \n",
    "from scipy.stats import bernoulli\n",
    "class Simulator:\n",
    "    \"\"\"\n",
    "    It represents the teaching system. Several students log into it to take courses. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the teaching system by loading data about students, topics & content. Also, initialize other objects to\n",
    "        be used with the learning system, such as the learning algorithm (LinUCB), the omniscient policy (Oracle) , skip topic\n",
    "        to optimize rewards, by skipping topics which have content with low expected rewards. \n",
    "        \"\"\"\n",
    "        self.context = Context()\n",
    "        self.context.setStudentContext()\n",
    "        self.context.setContentContext()\n",
    "        self.context.setTopic()\n",
    "        self.context.setTopicContent()\n",
    "        self.oracle = Oracle()\n",
    "        self.linUCB = LinUCB()\n",
    "        self.skipTopic = SkipTopic()\n",
    "        self.simulator_lock = threading.Lock()\n",
    "        self.rounds=0\n",
    "        self.rounds_interval = 1\n",
    "        self.logs = pd.DataFrame(columns = ['student_number','topic','arm_pulled','pay-off','pay-off_next_topic','actual_decision','skip_decision','skip_enabled'\n",
    "                                            ,'reward']) \n",
    "#         self.logs = pd.DataFrame(columns = ['student_number','topic','arm_pulled','reward']) \n",
    "\n",
    "\n",
    "    def getPayoff(self,X,arm,pta):\n",
    "        \"\"\"\n",
    "        Student shares feedback about the content / understanding of the topic. \n",
    "        \n",
    "        Input : \n",
    "        \n",
    "        X : Context vector for the round. \n",
    "        arm : Arm to be pulled / Content to be shown\n",
    "        pta : Payoff at round 't' for pulling arm 'a'\n",
    "        \n",
    "        Returns : \n",
    "        \n",
    "        reward : Reward / Feedback from student for the content shown / arm pulled\n",
    "        \"\"\"\n",
    "        arm_theta = self.oracle.theta_df.loc[arm] #Get parameters for the arm predicted by the learning algo\n",
    "        expected_reward = pd.Series.dot(X,arm_theta) # Vector dim : (1 * d) (d * 1).\n",
    "#         with self.simulator_lock:\n",
    "#             print(\"expected_reward : \",expected_reward)\n",
    "        reward = bernoulli.rvs(size=1,p=expected_reward)[0] # Simulate student's response    \n",
    "#         if reward == 0:\n",
    "#             reward = -1\n",
    "        return reward , expected_reward\n",
    "    \n",
    "    def takeCourse(self,student_number,studentContext,contentContext,topic,topic_content):\n",
    "        \"\"\"\n",
    "        This method simulates students taking a course. As part of it, students are presented content for various topics. \n",
    "        Students share their feedback, based on which we either move to the next topic or remain on the same topic.  \n",
    "        We get the expected pay-off from the learning algoritm. We then decide whether to skip or remain on the same topic.\n",
    "        If skip is true, then the student moves to the next topic, else the student remains on the same topic, shares feedback on \n",
    "        the content & we train the skip classifier with this feedback. This method drives the flow of the system, hence key data \n",
    "        elements available in this method are logged for analysis.\n",
    "        \n",
    "        Inputs : \n",
    "        \n",
    "        student_number : Student Id \n",
    "        studentContext : Student context vector. \n",
    "        contentContext : Contents context. This has context of all contents for the topic. \n",
    "        topic : All the topics to be taught as part of the course. \n",
    "        topic_content : Relates all topics to the contents available for every topic     \n",
    "         \n",
    "        \"\"\"\n",
    "        for i in topic:\n",
    "            skip_enabled = False\n",
    "            contents = topic_content[i] # You now have all arms associated with the topic 'i'\n",
    "            t_c = contentContext.loc[contents] # Get all arms/contents for topic 'i'\n",
    "            contexts = self.context.prepareContext(studentContext,t_c) # Prepare context for this round\n",
    "            arms = list(t_c.index) # Get a list of all arms\n",
    "            while arms: # While we still have arms remaining. \n",
    "                arm , pta = self.linUCB.expectedPayoff(contexts,arms) # Get me the max expected pay-off for this round & the arm that would give that reward / pay-off\n",
    "#                 with self.simulator_lock:\n",
    "# #                     print('PTA : ', pta)\n",
    "                actual_decision , confidence_score, skip_decision , pta_next_topic = self.skipTopic.skipTopic(studentContext,pta,i,self.context,topic_content,self.linUCB) # Check if makes sense to skip this topic & move to the next one\n",
    "                if skip_decision and skip_enabled: \n",
    "                    log = pd.Series([student_number,i,arm,pta,pta_next_topic,actual_decision,confidence_score,skip_decision,skip_enabled], \n",
    "                                        index=['student_number','topic','arm_pulled','pay-off',\n",
    "                                                'pay-off_next_topic','actual_decision','confidence_score','skip_decision','skip_enabled']) # Print log for this round\n",
    "                    with self.simulator_lock:\n",
    "#                         print('We\\'re skipping. Student {0} is on topic {1} was expected to be shown content {2}. Expected Pay-off of this arm is {3}, compared to expected pay-off of next round is {4}. Actual decision was {5} with confidence {6} Decision of skip classifier is {7}'\n",
    "#                           .format(student_number,i,arm,pta,pta_next_topic,actual_decision,confidence_score,skip_decision))                    \n",
    "                        self.logs = self.logs.append(log , ignore_index=True) # Log in a file\n",
    "                    break # Decision is to skip. Hence, we won't pull the arm.  \n",
    "                else:\n",
    "#                     actual_payoff , expected_reward = self.getPayoff(contexts.loc[arm],arm,pta) # Student shares feedback\n",
    "#                 log = pd.Series([student_number,i,arm,actual_payoff], \n",
    "#                                         index=['student_number','topic','arm_pulled','reward']) # Print log for this round\n",
    "#                 with self.simulator_lock:\n",
    "#                     self.rounds+=1\n",
    "#                     print('Student {0} is on topic {1} is shown content {2} feedback recd is {3}. '\n",
    "#                               .format(student_number,i,arm,actual_payoff))\n",
    "#                     self.logs = self.logs.append(log , ignore_index=True) # Log in a file\n",
    "                    actual_payoff , expected_reward = self.getPayoff(contexts.loc[arm],arm,pta) # Student shares feedback\n",
    "                    log = pd.Series([student_number,i,arm,pta,pta_next_topic,actual_decision,confidence_score,skip_decision,skip_enabled,actual_payoff], \n",
    "                                        index=['student_number','topic','arm_pulled','pay-off',\n",
    "                                                'pay-off_next_topic','actual_decision','confidence_score','skip_decision','skip_enabled','reward']) # Print log for this round\n",
    "                    with self.simulator_lock:\n",
    "                        self.rounds+=1\n",
    "                        if self.rounds > self.rounds_interval:\n",
    "                            print('{0} rounds completed'.format(self.rounds))\n",
    "                            self.rounds_interval += 1000        \n",
    "#                         print('Student {0} is on topic {1} is shown content {2} feedback recd is {3}. Expected Pay-off of this arm is {4}, compared to expected pay-off of next round is {5}. Actual decision was {6} with confidence {7}. Decision of skip classifier is {8} and skipping is {9}. Expected reward was {10}'\n",
    "#                               .format(student_number,i,arm,actual_payoff,pta,pta_next_topic,actual_decision,confidence_score,skip_decision,skip_enabled,expected_reward))\n",
    "                        self.logs = self.logs.append(log , ignore_index=True) # Log in a file\n",
    "                self.linUCB.updateParams(arm,contexts.loc[arm], actual_payoff) # Update arm parameters for this round\n",
    "                label = self.skipTopic.setLabel(actual_payoff) # Set label to train skip classifier\n",
    "                self.skipTopic.train(studentContext,pta,pta_next_topic,label) # Train skip classifier.\n",
    "                self.skipTopic.updateConfidenceThreshold(self.rounds)\n",
    "                if actual_payoff != 1: # If we got no reward for this round\n",
    "                    arms.remove(arm) #  # lets the that arm. \n",
    "                    skip_enabled = True\n",
    "                else:\n",
    "                    break # Move to the next topic \n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Its the main method. Its in the name :)\n",
    "        \"\"\"\n",
    "        studentContext = self.context.getStudentContext() # Student dataframe\n",
    "        contentContext = self.context.getContentContext() # Content Dataframe\n",
    "        topic = self.context.getTopic() # List of topics. \n",
    "        topic_content = self.context.getTopicContent() # Topics Data, which includes topics to content mapping.\n",
    "        features = list(studentContext.columns) + list(contentContext.columns)\n",
    "        self.oracle.setParameters(features , contentContext.index)\n",
    "        student_thread = list() # Keep track of students taking the course. \n",
    "        for student_number , student in studentContext.iterrows():\n",
    "            t = threading.Thread(target=self.takeCourse, args=(student_number,student,contentContext,topic,topic_content))\n",
    "            student_thread.append(t)\n",
    "            # Some threads do background tasks, like sending keepalive packets, or performing periodic garbage collection, or \n",
    "            # whatever. These are only useful when the main program is running, and it's okay to kill them off once the other, \n",
    "            # non-daemon, threads have exited. Once the main thread finishes & one of the student is still working through the course. \n",
    "            # we will wait for the student to complete the course, since the main thread is completed. We want all students \n",
    "            # to complete the course. Hence, setting daemon to False\n",
    "            t.daemon = False # classifying as a non-daemon, so they will npt die even when the main thread does. \n",
    "            t.start() # begins, must come after daemon definition\n",
    "        for t in student_thread: # This is done to ensure, we proceed to save the logs only after all students have completed the course. \n",
    "            t.join()\n",
    "        self.logs.to_csv('logs_linUCB_large',index=False)\n",
    "        print('Total Number of rounds : ', self.rounds)  \n",
    "\n",
    "simulator = Simulator()\n",
    "simulator.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiements \n",
    "\n",
    "1) $\\alpha$ = 2 . Confidence threshold = 100 (No skipping). very_small data size. Contents between 5 - 20 \n",
    "2) $\\alpha$ = 0.5 . Confi thresh = 100\n",
    "\n",
    "## Analysis \n",
    "\n",
    "- How much do the rounds increase of increasing values of $\\alpha$ ? \n",
    "- How well are contents explored for different values of $\\alpha$ ? This would show the need to not make $\\alpha$ too small or too big\n",
    "- \n",
    "\n",
    "## Evaluation \n",
    "\n",
    "#### Very Small - Upto 20 contents per topic. \n",
    "\n",
    "When the number of contents per topic are limited upto a certain range, the learning algorithm performs as well as the omniscient policy. However, as the number of contents per topic increases, it performs sub-optimally. $\\textit{Show the chart when number of contents per topic can be upto 20}$ \n",
    "\n",
    "When we enabled skipping & \n",
    "\n",
    "We found the algorithm performs on par with the omniscient policy when the number of contents per topic is upto 10. However, when the number of contents per topic goes beyond 10 its performance degrades. This is because it has to explore more contents.\n",
    "\n",
    "Skipping algorithm , learns in online mode. Its not been pre-trained. It would be interesting to see how it performs when its pre-trained. \n",
    "\n",
    "PS: Whenever we give range, its inclusive of both numbers specified in the range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- We'll refer to the learning algorithm as learner \n",
    "- After finding the best content for different context, the learner now knows the best arm to pull. We refer to this as exploitation. This is inline with Exploration - Exploitation dilemma explained in Chapter 2 \\ref{exploreExploit}. Exploration refers to finding the best content for different contexts, where as exploitation refers to using the knowledge acquired during exploration to present the best content for a student. \n",
    "- \n",
    "\n",
    "Since, we have created the dataset, we had the option to experiment against different combinations. \n",
    "\n",
    "- Very Small : 50 students with 10 topics\n",
    "- Small :  100 students with 25 topics\n",
    "- Medium : 200 students with 50 topics\n",
    "- Large : 400 students with 100 topics\n",
    "- Very Large : 800 students with 200 topics\n",
    "\n",
    "Students - Varied from 50 to 800\n",
    "\n",
    "Topics - Varied from 10 to 200\n",
    "\n",
    "Contents - When the number of contents per topic was less say upto 4, then the learning algorithm performed as well as the omniscient policy. This would be because there was limited exploration of contents & the learner was able to find the best content for every topic for different students relatively quickly. However, when the number of contents per topic was increased to say upto 20, then the learner performed sub-optimally. This was natural, as now there were many more contents to explore. Hence the learner would need more rounds to converge to the best content for every topic \\& student. \n",
    "\n",
    "$\\alpha$ : Having a higher value of $\\alpha , like 2.0 encourged the learner to explore more, before exploting. \n",
    "\n",
    "confidence_threshold : To disabled skipping set this threshold to 100. A low value on the skipping threshold could encourage skipping at the first attempt of failing to understand a topic. Hence, setting this threshold to an acceptable value, is the perogative of the instructor / teacher. \n",
    "\n",
    "Performance of skipping algorithm : The skipping algorithm is an online algorithm, which learns in real-time. \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Modelling non-stationary student responses is non-trivial as there are external factors beyond the control of the system. Concepts like attitude, study patterns, motivation are abstract concepts which are well understood, but cannot be measured / derived mathematically.\n",
    "\n",
    "## Exp\n",
    "\n",
    "- For small, make number of students as 25 & increases contents to 100. \n",
    "- \n",
    "\n",
    "## small size upto 20 contents. No skipping. \n",
    "\n",
    "We observe that the cumulative reward increases linearly for the omniscient policy. Compared to it, the learner's cumulative reward is sub-optimal, which is expected, as it has to learn the optimal parameters. \n",
    "\n",
    "## For different values of $\\alpha$ \n",
    "\n",
    "From experiments we found that setting $\\alpha$ too high reduces cumulative reward. This is because the learner spends more time exploring. However, setting a low value of $\\alpha$ prevents exploration \\& restricts learning, as the learner is not able to evaluate different contents / explanations. Based on empirical data, we found $\\alpha = 0.5$ to be optimal. This can be shown by the graph which shows that this value of alpha helps the learner increase its cumulative reward. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_number</th>\n",
       "      <th>topic</th>\n",
       "      <th>arm_pulled</th>\n",
       "      <th>pay-off</th>\n",
       "      <th>pay-off_next_topic</th>\n",
       "      <th>actual_decision</th>\n",
       "      <th>skip_decision</th>\n",
       "      <th>reward</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>expected_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>0.622314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>0.645088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>0.635686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.518954</td>\n",
       "      <td>0.567077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>67</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.588675</td>\n",
       "      <td>0.667801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.699233</td>\n",
       "      <td>0.513402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.540866</td>\n",
       "      <td>0.584603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>75</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.322840</td>\n",
       "      <td>0.568732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.012978</td>\n",
       "      <td>0.556236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.314895</td>\n",
       "      <td>0.669952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-31.253372</td>\n",
       "      <td>0.637477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.067302</td>\n",
       "      <td>0.715813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>77</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.848923</td>\n",
       "      <td>0.618286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>83</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.163952</td>\n",
       "      <td>0.645168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>79</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-24.142591</td>\n",
       "      <td>0.591547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>99</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-48.729693</td>\n",
       "      <td>0.591553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>88</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.149318</td>\n",
       "      <td>0.690087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-39.035715</td>\n",
       "      <td>0.606551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>22</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-38.830029</td>\n",
       "      <td>0.618324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>70</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.570299</td>\n",
       "      <td>0.575749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>92</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-46.641026</td>\n",
       "      <td>0.723002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>94</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-24.945545</td>\n",
       "      <td>0.582839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>37</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-28.748099</td>\n",
       "      <td>0.559001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>66</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.492107</td>\n",
       "      <td>0.575224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>16</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-42.575211</td>\n",
       "      <td>0.717212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>18</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-33.780515</td>\n",
       "      <td>0.644623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>26</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.024001</td>\n",
       "      <td>0.583064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>29</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.688378</td>\n",
       "      <td>0.583641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>87</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-29.314896</td>\n",
       "      <td>0.587488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>32</td>\n",
       "      <td>T_1</td>\n",
       "      <td>C_1_14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-49.481312</td>\n",
       "      <td>0.692074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>82</td>\n",
       "      <td>T_22</td>\n",
       "      <td>C_22_4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.414687</td>\n",
       "      <td>0.387990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>67</td>\n",
       "      <td>T_22</td>\n",
       "      <td>C_22_5</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.116661</td>\n",
       "      <td>0.540903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>35</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_7</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.297205</td>\n",
       "      <td>0.513328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>29</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.658984</td>\n",
       "      <td>0.420776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>42</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_6</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.050236</td>\n",
       "      <td>0.635658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>49</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.215339</td>\n",
       "      <td>0.442479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>26</td>\n",
       "      <td>T_22</td>\n",
       "      <td>C_22_8</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.593575</td>\n",
       "      <td>0.614299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>95</td>\n",
       "      <td>T_21</td>\n",
       "      <td>C_21_12</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.308203</td>\n",
       "      <td>0.483050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>29</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.886018</td>\n",
       "      <td>0.423042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>45</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_15</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.319050</td>\n",
       "      <td>0.521079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>70</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_9</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.967718</td>\n",
       "      <td>0.507612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>35</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_14</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.791119</td>\n",
       "      <td>0.486266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>95</td>\n",
       "      <td>T_21</td>\n",
       "      <td>C_21_10</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.455949</td>\n",
       "      <td>0.597706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>49</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_13</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.771953</td>\n",
       "      <td>0.461046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>67</td>\n",
       "      <td>T_22</td>\n",
       "      <td>C_22_3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.797589</td>\n",
       "      <td>0.603112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>45</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.624809</td>\n",
       "      <td>0.656950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>1</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.677305</td>\n",
       "      <td>0.438713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>79</td>\n",
       "      <td>T_22</td>\n",
       "      <td>C_22_2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.556180</td>\n",
       "      <td>0.540607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>1</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_9</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.673163</td>\n",
       "      <td>0.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>45</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9.440381</td>\n",
       "      <td>0.482678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>53</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_7</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.344077</td>\n",
       "      <td>0.466003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>49</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_14</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.361809</td>\n",
       "      <td>0.442922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>67</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_15</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.247865</td>\n",
       "      <td>0.586184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>95</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.719006</td>\n",
       "      <td>0.569575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>67</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.965438</td>\n",
       "      <td>0.603550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>67</td>\n",
       "      <td>T_23</td>\n",
       "      <td>C_23_12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.071013</td>\n",
       "      <td>0.753069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>95</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_7</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.610056</td>\n",
       "      <td>0.565184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>67</td>\n",
       "      <td>T_24</td>\n",
       "      <td>C_24_9</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.977296</td>\n",
       "      <td>0.555506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>95</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_13</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.881340</td>\n",
       "      <td>0.501648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4069</th>\n",
       "      <td>67</td>\n",
       "      <td>T_25</td>\n",
       "      <td>C_25_14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.231094</td>\n",
       "      <td>0.488000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1985 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      student_number topic arm_pulled  pay-off  pay-off_next_topic  \\\n",
       "0                  2   T_1     C_1_14     0.44                0.48   \n",
       "1                  4   T_1     C_1_14     0.43                0.48   \n",
       "2                  0   T_1     C_1_14     0.44                0.48   \n",
       "5                 53   T_1     C_1_14     0.42                0.46   \n",
       "6                 67   T_1     C_1_14     0.45                0.49   \n",
       "7                  7   T_1     C_1_14     0.41                0.45   \n",
       "9                 74   T_1     C_1_14     0.42                0.46   \n",
       "15                75   T_1     C_1_14     0.45                0.46   \n",
       "16                19   T_1     C_1_14     0.45                0.46   \n",
       "17                14   T_1     C_1_14     0.48                0.48   \n",
       "20                48   T_1     C_1_14     0.47                0.48   \n",
       "21                64   T_1     C_1_14     0.48                0.50   \n",
       "22                77   T_1     C_1_14     0.46                0.47   \n",
       "23                83   T_1     C_1_14     0.44                0.48   \n",
       "24                79   T_1     C_1_14     0.45                0.47   \n",
       "25                99   T_1     C_1_14     0.44                0.48   \n",
       "26                88   T_1     C_1_14     0.48                0.50   \n",
       "28                76   T_1     C_1_14     0.46                0.47   \n",
       "31                22   T_1     C_1_14     0.47                0.48   \n",
       "44                70   T_1     C_1_14     0.46                0.46   \n",
       "46                92   T_1     C_1_14     0.40                0.50   \n",
       "48                94   T_1     C_1_14     0.47                0.47   \n",
       "49                37   T_1     C_1_14     0.43                0.45   \n",
       "50                66   T_1     C_1_14     0.45                0.47   \n",
       "51                16   T_1     C_1_14     0.38                0.50   \n",
       "52                18   T_1     C_1_14     0.45                0.48   \n",
       "53                26   T_1     C_1_14     0.47                0.48   \n",
       "55                29   T_1     C_1_14     0.45                0.47   \n",
       "56                87   T_1     C_1_14     0.47                0.48   \n",
       "58                32   T_1     C_1_14     0.47                0.50   \n",
       "...              ...   ...        ...      ...                 ...   \n",
       "3977              82  T_22     C_22_4     0.29                0.29   \n",
       "3981              67  T_22     C_22_5     0.44                0.22   \n",
       "3982              35  T_24     C_24_7     0.33                0.28   \n",
       "3989              29  T_25    C_25_12     0.26                0.00   \n",
       "3990              42  T_24     C_24_6     0.31                0.26   \n",
       "3991              49  T_23    C_23_15     0.30                0.33   \n",
       "3992              26  T_22     C_22_8     0.34                0.27   \n",
       "3993              95  T_21    C_21_12     0.35                0.35   \n",
       "3996              29  T_25    C_25_10     0.30                0.00   \n",
       "3997              45  T_23    C_23_15     0.30                0.32   \n",
       "3998              70  T_24     C_24_9     0.27                0.28   \n",
       "4006              35  T_25    C_25_14     0.30                0.00   \n",
       "4007              95  T_21    C_21_10     0.29                0.35   \n",
       "4008              49  T_23    C_23_13     0.29                0.34   \n",
       "4009              67  T_22     C_22_3     0.42                0.23   \n",
       "4022              45  T_24     C_24_6     0.32                0.29   \n",
       "4034               1  T_25    C_25_14     0.25                0.00   \n",
       "4036              79  T_22     C_22_2     0.30                0.27   \n",
       "4037               1  T_25     C_25_9     0.24                0.00   \n",
       "4038              45  T_25    C_25_14     0.29                0.00   \n",
       "4040              53  T_24     C_24_7     0.28                0.29   \n",
       "4041              49  T_25    C_25_14     0.32                0.00   \n",
       "4043              67  T_23    C_23_15     0.23                0.29   \n",
       "4050              95  T_23    C_23_16     0.30                0.35   \n",
       "4051              67  T_23    C_23_14     0.25                0.30   \n",
       "4057              67  T_23    C_23_12     0.26                0.30   \n",
       "4061              95  T_24     C_24_7     0.34                0.29   \n",
       "4062              67  T_24     C_24_9     0.30                0.25   \n",
       "4068              95  T_25    C_25_13     0.29                0.00   \n",
       "4069              67  T_25    C_25_14     0.25                0.00   \n",
       "\n",
       "      actual_decision  skip_decision  reward  confidence_score  \\\n",
       "0                   0              0     1.0        -10.000000   \n",
       "1                   0              0     1.0        -10.000000   \n",
       "2                   0              0     0.0        -10.000000   \n",
       "5                   0              0     0.0        -10.518954   \n",
       "6                   0              0     1.0         -2.588675   \n",
       "7                   0              0     0.0         -5.699233   \n",
       "9                   0              0     0.0         -5.540866   \n",
       "15                  0              0     0.0        -14.322840   \n",
       "16                  0              0     1.0         -4.012978   \n",
       "17                  0              0     1.0         -9.314895   \n",
       "20                  0              0     1.0        -31.253372   \n",
       "21                  0              0     0.0         -4.067302   \n",
       "22                  0              0     0.0         -1.848923   \n",
       "23                  0              0     0.0        -19.163952   \n",
       "24                  0              0     0.0        -24.142591   \n",
       "25                  0              0     1.0        -48.729693   \n",
       "26                  0              0     0.0         -3.149318   \n",
       "28                  0              0     1.0        -39.035715   \n",
       "31                  0              0     1.0        -38.830029   \n",
       "44                  0              0     0.0        -30.570299   \n",
       "46                  0              0     1.0        -46.641026   \n",
       "48                  0              0     1.0        -24.945545   \n",
       "49                  0              0     1.0        -28.748099   \n",
       "50                  0              0     1.0        -35.492107   \n",
       "51                  0              0     1.0        -42.575211   \n",
       "52                  0              0     0.0        -33.780515   \n",
       "53                  0              0     0.0        -34.024001   \n",
       "55                  0              0     0.0        -34.688378   \n",
       "56                  0              0     0.0        -29.314896   \n",
       "58                  0              0     0.0        -49.481312   \n",
       "...               ...            ...     ...               ...   \n",
       "3977                0              0     0.0         -6.414687   \n",
       "3981                0              0     0.0         -4.116661   \n",
       "3982                0              0     1.0         -6.297205   \n",
       "3989                0              0     0.0        -12.658984   \n",
       "3990                0              0     1.0         -8.050236   \n",
       "3991                0              0     0.0         -4.215339   \n",
       "3992                0              0     1.0         -1.593575   \n",
       "3993                0              0     0.0         -8.308203   \n",
       "3996                0              0     0.0         -3.886018   \n",
       "3997                0              0     1.0         -7.319050   \n",
       "3998                0              0     0.0         -8.967718   \n",
       "4006                0              0     1.0         -9.791119   \n",
       "4007                0              0     1.0         -7.455949   \n",
       "4008                0              0     1.0         -2.771953   \n",
       "4009                0              0     0.0         -0.797589   \n",
       "4022                0              0     1.0         -1.624809   \n",
       "4034                0              0     0.0         -4.677305   \n",
       "4036                0              0     0.0         -6.556180   \n",
       "4037                0              0     1.0         -1.673163   \n",
       "4038                0              0     1.0         -9.440381   \n",
       "4040                0              0     1.0         -1.344077   \n",
       "4041                0              0     0.0         -2.361809   \n",
       "4043                0              0     0.0         -0.247865   \n",
       "4050                0              0     1.0         -2.719006   \n",
       "4051                0              0     0.0         -2.965438   \n",
       "4057                0              0     1.0         -2.071013   \n",
       "4061                0              0     1.0         -3.610056   \n",
       "4062                0              0     1.0         -1.977296   \n",
       "4068                0              0     1.0         -3.881340   \n",
       "4069                0              0     1.0         -2.231094   \n",
       "\n",
       "      expected_reward  \n",
       "0            0.622314  \n",
       "1            0.645088  \n",
       "2            0.635686  \n",
       "5            0.567077  \n",
       "6            0.667801  \n",
       "7            0.513402  \n",
       "9            0.584603  \n",
       "15           0.568732  \n",
       "16           0.556236  \n",
       "17           0.669952  \n",
       "20           0.637477  \n",
       "21           0.715813  \n",
       "22           0.618286  \n",
       "23           0.645168  \n",
       "24           0.591547  \n",
       "25           0.591553  \n",
       "26           0.690087  \n",
       "28           0.606551  \n",
       "31           0.618324  \n",
       "44           0.575749  \n",
       "46           0.723002  \n",
       "48           0.582839  \n",
       "49           0.559001  \n",
       "50           0.575224  \n",
       "51           0.717212  \n",
       "52           0.644623  \n",
       "53           0.583064  \n",
       "55           0.583641  \n",
       "56           0.587488  \n",
       "58           0.692074  \n",
       "...               ...  \n",
       "3977         0.387990  \n",
       "3981         0.540903  \n",
       "3982         0.513328  \n",
       "3989         0.420776  \n",
       "3990         0.635658  \n",
       "3991         0.442479  \n",
       "3992         0.614299  \n",
       "3993         0.483050  \n",
       "3996         0.423042  \n",
       "3997         0.521079  \n",
       "3998         0.507612  \n",
       "4006         0.486266  \n",
       "4007         0.597706  \n",
       "4008         0.461046  \n",
       "4009         0.603112  \n",
       "4022         0.656950  \n",
       "4034         0.438713  \n",
       "4036         0.540607  \n",
       "4037         0.385332  \n",
       "4038         0.482678  \n",
       "4040         0.466003  \n",
       "4041         0.442922  \n",
       "4043         0.586184  \n",
       "4050         0.569575  \n",
       "4051         0.603550  \n",
       "4057         0.753069  \n",
       "4061         0.565184  \n",
       "4062         0.555506  \n",
       "4068         0.501648  \n",
       "4069         0.488000  \n",
       "\n",
       "[1985 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('logs_linUCB_small')\n",
    "df[df['confidence_score'] < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
